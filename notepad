perpetual notepad: i keep adding to this whenever.

april 9

i'm starting a machine learning course on coursera. this is like the third time i've started up, every time i quit midway. hopefully i dont quit

implemented better graphics, functions such as mathematical loops and effects into unlimited-bullets-festival

got touhou soundtrack: its better with all the bullet noises

april 10

	coursera machine learning notes:

		linear regression: finding the line that fits all the points

		functions at http://puu.sh/h90JM/a14f7baef7.png.

		cost function is the 1/2m thing

		m = number of training examples

		in the loop: i is the current training example

		h.theta(x(i)) is the same as theta.0 + theta.1*x(i).
		theta.1 is the slope, and x(i) is just the current x.

		The cost function: the average of the hypothesis(base and slope) differnce between the actual numbers.
		we are trying to make the cost function as low as possible.


	ideas for fixing up skyflower: make it more bullet-helly. minibosses that spam bullets everywhere, make the player hitbox smaller, add turrets that spray patterns everywhere, fix up the bosses to have more interesting patterns.

	stanford course:
		set: just a bunch of stuff in some kind of order, no duplicates
			hashset: no order
			multiply sets = returns things that are in both sets
		lexicon
		map: pairs with key/value: dictonary -> indexes are keys, can use brackets



	more coursera ml notes:

		gradient descent: keep moving in small steps until you reach the optimum

			alpha: how big the step is
			simultanieous update: find theta.0 and theta.1 with the same function, then update both for the next step.
			when its setting theta.j: it means it sets theta.0 and theta.1, because it loops as j.
			derivatives: at a certain point, the slope of the curve.
				FOR THE 3D: we take derivates of both theta.0 and theta.1 in the simultaneous update.

			remember how sometimes this can screw up if there are two local optima? well with our cost function theres only one, so its fine.
